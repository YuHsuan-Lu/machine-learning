{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"10\">Mehta Purvil</font></center>\n",
    "<br><center><font size=\"6\">Github Username: purvilmehta06</font></center>\n",
    "<br><center><font size=\"6\">USC ID: 6104797766</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bootstrap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, roc_auc_score, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import resample\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = '../data/AReM/'\n",
    "columns = ['avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "all_features = ['min', 'max', 'mean', '50%', 'std', '25%', '75%']\n",
    "all_features_names = ['min', 'max', 'mean', 'median', 'std', '1st_quartile', '3rd_quartile']\n",
    "classes_ = {'bending1': 0, 'bending2': 1, 'cycling': 2, 'lying': 3, 'sitting': 4, 'standing': 5, 'walking': 6}\n",
    "rev_classes_ = {0: 'bending1', 1: 'bending2', 2: 'cycling', 3: 'lying', 4: 'sitting', 5: 'standing', 6: 'walking'}\n",
    "binary_class_dict = {1: 'bending', 0: 'non-bending'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Get the AReM Data Set and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '../data/AReM/bendingType.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_folder_path):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m folder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.DS_Store\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_folder_path \u001b[38;5;241m+\u001b[39m folder):\n\u001b[1;32m      5\u001b[0m         key \u001b[38;5;241m=\u001b[39m data_folder_path \u001b[38;5;241m+\u001b[39m folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m file\n\u001b[1;32m      6\u001b[0m         files[key] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(key, skiprows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, usecols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m])\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '../data/AReM/bendingType.pdf'"
     ]
    }
   ],
   "source": [
    "files = {}\n",
    "for folder in os.listdir(data_folder_path):\n",
    "    if folder == '.DS_Store': continue\n",
    "    for file in os.listdir(data_folder_path + folder):\n",
    "        key = data_folder_path + folder + '/' + file\n",
    "        files[key] = pd.read_csv(key, skiprows = 4, usecols = [0, 1, 2, 3, 4, 5, 6])\n",
    "        del files[key][files[key].columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {}\n",
    "train_data = {}\n",
    "for folder in os.listdir(data_folder_path):\n",
    "    if folder == '.DS_Store': continue\n",
    "    for file in os.listdir(data_folder_path + folder):\n",
    "        file_name = data_folder_path + folder + '/' + file\n",
    "        if (folder == 'bending1' or folder == 'bending2'):\n",
    "            if file == 'dataset1.csv' or file == 'dataset2.csv':\n",
    "                test_data[file_name] = files[file_name]\n",
    "            else:\n",
    "                train_data[file_name] = files[file_name]\n",
    "        else:\n",
    "            if file == 'dataset1.csv' or file == 'dataset2.csv' or file == 'dataset3.csv':\n",
    "                test_data[file_name] = files[file_name]\n",
    "            else:\n",
    "                train_data[file_name] = files[file_name]\n",
    "\n",
    "print(\"Total Train Dataset Files: \", len(train_data))\n",
    "print(\"Total Test Dataset Files: \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### i. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In time series classification, time-domain features are extracted from the raw data to capture the underlying patterns and characteristics of the time series. Some of the commonly used time-domain features in time series classification are:\n",
    "\n",
    "* Mean: The arithmetic mean of the values in the time series.\n",
    "* Standard deviation: A measure of the variability or spread of the time series values.\n",
    "* Skewness: A measure of the asymmetry of the distribution of the time series values.\n",
    "* Variance: A measure of the dispersion of the time series values around the mean.\n",
    "* Interquartile range: The difference between the 75th and 25th percentiles of the time series values.\n",
    "* Cross Correlation - Cross-correlation is another commonly used time-domain feature in time series classification. It measures the similarity between two time series by computing the correlation between them at different time lags. Cross-correlation can help identify patterns that are similar between two time series, even if they are not aligned in time. \n",
    "* Autocorrelation: Autocorrelation is a measure of the correlation between a time series and a lagged version of itself, and can help identify repeating patterns or cycles within the time series. \n",
    "* Autoregressive (AR) - These models are a type of time series model that use past values of a time series to predict future values. The order of the AR model, denoted by p, specifies how many past values of the time series are used in the model.\n",
    "* Other parameters of ARIMA (Autoregressive integrated moving average) model like Integrated (I) and Moving Average (MA) are also some of the important features of any Time series data.\n",
    "\n",
    "These are just some examples of the many time-domain features that can be used in time series classification. The specific choice of features may depend on the nature of the data and the particular classification problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ii. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = ['min', 'max', 'mean', 'median', 'std', '1quartile', '3quartile']\n",
    "parameters = ['min', 'max', 'mean', '50%', 'std', '25%', '75%']\n",
    "extract_data = []\n",
    "for k, v in files.items():\n",
    "    stats = files[k].describe()\n",
    "    temp = {}\n",
    "    for feature in list(files[k].columns):\n",
    "        for parameter in parameters:\n",
    "            temp[feature + '_' + parameter] = stats[feature][parameter] \n",
    "    extract_data.append(temp)\n",
    "extract_data = pd.DataFrame(extract_data)\n",
    "extract_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "std_extract_data = []\n",
    "stds = []\n",
    "for col in list(extract_data.columns):\n",
    "    stds.append(extract_data[col].std())\n",
    "    std_extract_data.append([col, stds[-1]])\n",
    "std_extract_data = pd.DataFrame(std_extract_data, columns = ['Feature', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval = []\n",
    "for i, col in enumerate(list(extract_data.columns)):\n",
    "    bootResult = bootstrap((extract_data[col], ), np.std, confidence_level = 0.9, \n",
    "                              random_state = 1, method = 'percentile')\n",
    "\n",
    "    confidence_interval.append([col, stds[i], bootResult.confidence_interval.low, \n",
    "                                bootResult.confidence_interval.high])\n",
    "\n",
    "confidence_interval = pd.DataFrame(confidence_interval, \n",
    "                                   columns = ['Feature Name', 'Standard Deviation', 'Lower Bound', 'Upper Bound'])\n",
    "confidence_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### iv. Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Mean**: Considering many outliers, mean is one of the good measures to identify central (i.e. common) tendency when the data distribution is continuous.\n",
    "2. **Median**: Since the data has too much outliers, and outliers can change the overall mean value of the data towards them, median could be the good feature of central (i.e. common) tendency that is less affected by outliers.\n",
    "3. **Standard Deviation**: According to the defination, standard deviation is a measure to identify datapoints that are away from the mean. It also help us identify the underlying data distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Time Series Classification Part 2: Binary and Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Binary Classification Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(columns, selected_feature_name, l):\n",
    "    ans = []\n",
    "    for split_no in range(l):\n",
    "        for col in columns:\n",
    "            for f in selected_feature_name:\n",
    "                ans += [col + '_' + f + '_split_' + str(split_no + 1)]\n",
    "    ans += ['type']\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_key(key, is_binary):   \n",
    "    key = key.split('/')[3]\n",
    "    if is_binary:\n",
    "        return 1 if 'bending' in key else 0\n",
    "    else:\n",
    "        return classes_[key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_data, l, features, columns, selected_feature, is_binary = True):\n",
    "    \n",
    "    train_df = []\n",
    "    for k, v in train_data.items():\n",
    "        splits = np.array_split(v, l)\n",
    "        temp = []\n",
    "        for s in splits:\n",
    "            stat = s.describe()\n",
    "            for series in columns:\n",
    "                for feature in selected_feature:\n",
    "                    temp.append(stat[series][feature])\n",
    "\n",
    "        temp.append(get_class_from_key(k, is_binary))\n",
    "        train_df.append(temp)\n",
    "        \n",
    "    train_df = pd.DataFrame(train_df, columns = features)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_features_from_logistic_regression(features, selector):\n",
    "    selected_features = []\n",
    "    for i, feature in enumerate(features):\n",
    "        if selector.support_[i]:\n",
    "            selected_features.append(feature)\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_imbalance_class(train_df):\n",
    "    \n",
    "    total0 = train_df.loc[train_df['type'] == 0].shape[0]\n",
    "    total1 = train_df.loc[train_df['type'] == 1].shape[0]\n",
    "    minor_count = min(total0, total1)\n",
    "    major_count = max(total0, total1)\n",
    "    minor_label, major_label = (1, 0) if total0 > total1 else (0, 1)\n",
    "        \n",
    "    minor_ds = train_df[train_df['type'] == minor_label]\n",
    "    major_ds = train_df[train_df['type'] == major_label]\n",
    "    \n",
    "    minor_ds = resample(minor_ds, replace = True, random_state = 60, n_samples = max(total0, total1))\n",
    "    train_df = pd.concat([major_ds, minor_ds])\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train_data, test_data, l, fold_count, columns, \n",
    "                        selected_feature_name, selected_feature, oversampling = False):\n",
    "\n",
    "    # get features and data\n",
    "    features = get_features(columns, selected_feature_name, l)\n",
    "    train_df = get_data(train_data, l, features, columns, selected_feature)    \n",
    "    \n",
    "    # oversampling to prevent imbalance\n",
    "    if oversampling: \n",
    "        train_df = oversample_imbalance_class(train_df)\n",
    "    \n",
    "    # spliting data to train X and label Y\n",
    "    trainX = train_df.iloc[:, :-1]\n",
    "    trainY = train_df.iloc[:, -1]\n",
    "\n",
    "    # main model\n",
    "    validator = StratifiedKFold(n_splits = fold_count, shuffle = True, random_state = 69)\n",
    "    model = LogisticRegression(penalty = 'none', solver = 'lbfgs', max_iter = 100)\n",
    "    selector = RFECV(estimator = model, cv = validator, scoring = 'accuracy')\n",
    "    selector.fit(trainX, trainY)\n",
    "    \n",
    "    test_accuracy = selector.cv_results_['mean_test_score'].max()\n",
    "    \n",
    "    return model, selector, round(test_accuracy, 5), round(1 - test_accuracy, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(true_y, pred_y_prob, model):\n",
    "    \n",
    "    fig, axes = plt.subplots(figsize = (6, 4))\n",
    "    axes.set_title(\"ROC curves\")\n",
    "    for i, class_ in enumerate(model.classes_):\n",
    "        y_pred = pred_y_prob[:, i]\n",
    "        fpr, tpr, thresholds = roc_curve(true_y, y_pred, pos_label = class_)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        if (len(model.classes_) == 2):\n",
    "            axes.plot(fpr, tpr, label = \"Class: {}; AUC: {}\".format(binary_class_dict[class_], round(roc_auc, 4)))\n",
    "        else: \n",
    "            axes.plot(fpr, tpr, label = \"Class: {}; AUC: {}\".format(rev_classes_[class_], round(roc_auc, 4)))\n",
    "    \n",
    "    axes.plot([0, 1], [0, 1], \"--\")\n",
    "    axes.set_xlabel(\"False Positive Rate\")\n",
    "    axes.set_ylabel(\"True Positive Rate\")\n",
    "    axes.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm_roc_plot(model, trainX, trainY, selected_features, toggle = False):\n",
    "    \n",
    "    if not toggle:\n",
    "        model.fit(trainX.loc[:, selected_features], trainY.to_numpy(dtype = \"int\"))\n",
    "        \n",
    "    pred_y = model.predict(trainX.loc[:, selected_features])\n",
    "    pred_y_prob = model.predict_proba(trainX.loc[:, selected_features])\n",
    "    pred_y = pred_y.round()\n",
    "    true_y = trainY.to_numpy(dtype = float)\n",
    "    t_score = model.score(trainX.loc[:, selected_features], trainY.to_numpy(dtype = \"int\"))\n",
    "    print(\"Accuracy: {}\".format(t_score))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(true_y, pred_y))\n",
    "    plot_roc(true_y, pred_y_prob, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']\n",
    "selected_time_series = ['avg_rss12', 'var_rss12', 'var_rss23']\n",
    "headers = get_features(selected_time_series, selected_feature_names, 1)      \n",
    "train_data_1 = get_data(train_data, 1, headers, selected_time_series, selected_features)    \n",
    "sns.pairplot(data = train_data_1, hue = 'type', palette = {1: 'red', 0: 'blue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Splitted Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_time_series = ['avg_rss12', 'var_rss12', 'var_rss23']\n",
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']\n",
    "headers = get_features(selected_time_series, selected_feature_names, 2) \n",
    "train_data_1 = get_data(train_data, 2, headers, selected_time_series, selected_features)    \n",
    "sns.pairplot(data = train_data_1, hue = 'type', palette = {1: 'red', 0: 'blue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings \n",
    "\n",
    "After we break each time series into two equal-length time series, both of the results seem to be almost the same on first glance. There is no significant difference between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as per the piazza post - https://piazza.com/class/lcplfg0c8cn1ep/post/254 \n",
    "# using all time series with selected features \n",
    "\n",
    "cv_factor, L = 5, 20\n",
    "result = {'L': [], 'p': [], 'Train Score': [], 'Train Error': []}\n",
    "for l in range(1, L + 1):\n",
    "    print(\"Running logistic regression for l = {}\".format(l), end = '\\r')\n",
    "    model, selector, score, error = logistic_regression(train_data, test_data, l, cv_factor, columns, \n",
    "                                                 selected_feature_names, selected_features)\n",
    "    result['L'].append(l)\n",
    "    result['p'].append(selector.n_features_)\n",
    "    result['Train Score'].append(score)\n",
    "    result['Train Error'].append(error)\n",
    "\n",
    "result = pd.DataFrame(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "* We can clearly observe from the above table that best (l, p) pair is (7, 8). I have selected mean, median and std features to do my analysis. \n",
    "* The purpose of cross-validation is to estimate the error on test data. We do not want to deprive our algorithm with the knowledge of the various predictors available at the disposal of our dataset. **Hence, the correct way would be to do the Recursive Feature Elimination (RFE) along with Cross Validation.** The other way round, i.e. **RFE before performing Cross Validation is the wrong way.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Final logistic regression model and its confusion matrix, ROC and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = result['Train Score'].argmax()\n",
    "best_l = result['L'][idx]\n",
    "print(\"Best pair: ({}, {})\\n\".format(best_l, result['p'][idx]))\n",
    "l, cv_factor = result['Train Score'].argmax() + 1, 5\n",
    "features = get_features(columns, selected_feature_names, l)\n",
    "model, selector, score, error = logistic_regression(train_data, test_data, l, cv_factor, columns, \n",
    "                                                 selected_feature_names, selected_features)\n",
    "\n",
    "RFE_selected_features = selected_features_from_logistic_regression(features[:-1], selector)\n",
    "print(\"RFE selected features: \", RFE_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# p value analysis using stat model\n",
    "features = get_features(columns, selected_feature_names, l)\n",
    "train_df = get_data(train_data, l, features, columns, selected_features)  \n",
    "trainX, trainY = train_df.iloc[:, :-1], train_df.iloc[:, -1]\n",
    "df_temp = trainX.loc[:, RFE_selected_features]\n",
    "const_val = sm.add_constant(df_temp.to_numpy(dtype = float))\n",
    "model2 = sm.Logit(trainY.to_numpy(dtype = float), const_val)\n",
    "summary = model2.fit(method = 'bfgs').summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings: \n",
    "* As per the above p-value analysis and note underneath, we can say that the data is completely seperated. \n",
    "* This error is generally observed with logistic regression when data is seperated and hence p-value estimates can't be reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cm_roc_plot(model, trainX, trainY, RFE_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### v. Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']\n",
    "\n",
    "features = get_features(columns, selected_feature_names, best_l)\n",
    "test_df = get_data(test_data, l, features, columns, selected_features)    \n",
    "\n",
    "testX, testY = test_df.iloc[:, :-1], test_df.iloc[:, -1]\n",
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cm_roc_plot(model, testX, testY, RFE_selected_features, toggle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* The same test & train accuracy is observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### vi. Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the p-value analysis using statmodel suggests that there is complete separation. In this case, the Maximum Likelihood Estimator (MLE) doesn't exist and the parameters are not identified.\n",
    "* Because of this, it is possible that the well-separation of the classes is cauisng the instability in calculation o fthe regression parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### vii. Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yes, the data has an imbalance in it. **Class 0, i.e. non-bending points are 60. Whereas bending datapoints or true class is just 9. This means that data has imbalance.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']\n",
    "cv_factor, L = 5, 20\n",
    "result = {'L': [], 'p': [], 'Train Score': [], 'Train Error': []}\n",
    "for l in range(1, L + 1):\n",
    "    print('Running logistic regression with balanced for l = {}'. format(l), end = '\\r')\n",
    "    model, selector, score, error = logistic_regression(train_data, test_data, l, cv_factor, columns,\n",
    "                                                 selected_feature_names, selected_features, oversampling = True)\n",
    "    result['L'].append(l)\n",
    "    result['p'].append(selector.n_features_)\n",
    "    result['Train Score'].append(score)\n",
    "    result['Train Error'].append(error)\n",
    "\n",
    "result = pd.DataFrame(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = result['Train Score'].argmax()\n",
    "best_l, max_train_accuracy, fold_count = result['L'][idx], result['Train Score'][idx], 5\n",
    "print(\"Best model with l = {}, max_train_accuracy = {}\\n\".format(best_l, max_train_accuracy))\n",
    "features = get_features(columns, selected_feature_names, best_l)\n",
    "model, selector, score, error = logistic_regression(train_data, test_data, best_l, cv_factor, columns,\n",
    "                                                 selected_feature_names, selected_features, oversampling = True)\n",
    "\n",
    "RFE_selected_features = selected_features_from_logistic_regression(features[:-1], selector)\n",
    "print(\"Selected features: \", RFE_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(columns, selected_feature_names, best_l)\n",
    "train_df = get_data(train_data, best_l, features, columns, selected_features)    \n",
    "train_df = oversample_imbalance_class(train_df)\n",
    "trainX, trainY = train_df.iloc[:, :-1], train_df.iloc[:, -1]\n",
    "print(\"ROC Curve for train data\")\n",
    "get_cm_roc_plot(model, trainX, trainY, RFE_selected_features)\n",
    "print(\"ROC Curve for test data\")\n",
    "get_cm_roc_plot(model, testX, testY, RFE_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Binary Classification Using L1-penalized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### i. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndex(model, find_C_in_dict):\n",
    "    for i, c in enumerate(model.Cs_):\n",
    "        if (c == find_C_in_dict): \n",
    "            return i\n",
    "\n",
    "# utility  method for performing regression based on the given classifier\n",
    "def perform_classification(train_data, test_data, l, columns, selected_feature_name, selected_feature, \n",
    "                         cv_factor = None, is_plot_roc = False,\n",
    "                         is_binary = False, is_gaussian = False, is_multinomial_prior = False):\n",
    "    \n",
    "    features = get_features(columns, selected_feature_name, l)\n",
    "    test_df = get_data(test_data, l, features, columns, selected_feature, is_binary = is_binary)  \n",
    "    train_df = get_data(train_data, l, features, columns, selected_feature, is_binary = is_binary)\n",
    "        \n",
    "    # splitting labels from training data\n",
    "    trainX, trainY = train_df.iloc[:, :-1], train_df.iloc[:, -1]\n",
    "    testX, testY = test_df.iloc[:, :-1], test_df.iloc[:, -1]\n",
    "    random_state = 69 if cv_factor != None else None\n",
    "    \n",
    "    # main model for binary and multinomial\n",
    "    if is_binary:\n",
    "        model = LogisticRegressionCV(penalty = 'l1', max_iter = 100, scoring = 'accuracy',\n",
    "                                     cv = cv_factor, random_state = random_state, solver = 'liblinear')  \n",
    "        model.fit(trainX, trainY)\n",
    "        idx = getIndex(model, model.C_[0])\n",
    "        train_score = np.mean(model.scores_[1][:, idx])\n",
    "    elif is_gaussian:\n",
    "        model = GaussianNB()\n",
    "        if (is_plot_roc):\n",
    "            model.fit(trainX, trainY)\n",
    "        score = cross_val_score(estimator = model,X = trainX, y = trainY, \n",
    "                                cv = StratifiedKFold(n_splits = 5, random_state = 0, shuffle = True), \n",
    "                                scoring = \"accuracy\")\n",
    "        train_score = np.mean(score)\n",
    "    elif is_multinomial_prior:\n",
    "        model = MultinomialNB()\n",
    "        if (is_plot_roc):\n",
    "            model.fit(trainX, trainY)\n",
    "        score = cross_val_score(estimator = model,X = trainX, y = trainY, \n",
    "                                cv = StratifiedKFold(n_splits = 5, random_state = 0, shuffle = True), \n",
    "                                scoring = \"accuracy\")\n",
    "        train_score = np.mean(score)\n",
    "        \n",
    "    else:\n",
    "        model = LogisticRegressionCV(penalty = 'l1', max_iter = 100, multi_class = 'multinomial', \n",
    "                                     cv = cv_factor, random_state = random_state, solver = 'saga') \n",
    "        model.fit(trainX, trainY)\n",
    "        idx = getIndex(model, model.C_[0])\n",
    "        score = 0\n",
    "        for class_ in model.scores_.keys():\n",
    "            score += np.mean(model.scores_[class_][:, idx])\n",
    "        train_score = score/len(model.scores_.keys())\n",
    "    train_error = round((1 - train_score), 4)\n",
    "    \n",
    "    \n",
    "    # printing results as per the need\n",
    "    if not is_plot_roc:    \n",
    "        return [l, train_score, train_error]\n",
    "    else:\n",
    "        \n",
    "        train_predict = model.predict_proba(trainX)\n",
    "        test_predict = model.predict_proba(testX)\n",
    "        pred_y = model.predict(testX)\n",
    "        cm = confusion_matrix(testY, pred_y)\n",
    "        \n",
    "        print(\"Confusion matrix for test data:\\n{}\".format(cm))\n",
    "        print(\"\\nROC Curve for Train Data\")\n",
    "        plot_roc(trainY, train_predict, model)\n",
    "        \n",
    "        print(\"\\nROC Curve for Test Data\")\n",
    "        plot_roc(testY, test_predict, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making use of the varargs concept\n",
    "cv_factor, total_l, result = 5, 20, []\n",
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']\n",
    "\n",
    "global max_train_accuracy, best_l\n",
    "max_train_accuracy, best_l = 0, 1\n",
    "\n",
    "# performing regression for l = 1 to 20\n",
    "for l in range(1, total_l + 1):\n",
    "    \n",
    "    print(\"Running binary logistic regression with l = {}\".format(l), end = '\\r')\n",
    "    result.append(perform_classification(train_data, test_data, l, columns, selected_feature_names, \n",
    "                                     selected_features, cv_factor = cv_factor, is_binary = True))\n",
    "    if result[-1][1] > max_train_accuracy:\n",
    "        max_train_accuracy = result[-1][1]\n",
    "        best_l = l\n",
    "\n",
    "result = pd.DataFrame(result, columns = ['L', 'Train Score', 'Train Error'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best model with l = {}, max_train_accuracy = {}\\n\".format(best_l, max_train_accuracy))\n",
    "print(\"Performing the analysis on test data with best l = {}\\n\".format(best_l))\n",
    "perform_classification(train_data, test_data, best_l, columns, selected_feature_names, selected_features, \n",
    "                   is_binary = True, is_plot_roc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ii. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The L1-regularization logistic regression has worse performance when it comes to accuracy on the test data and hence, **the L1-regularization performs worse than RFECV logistic regression. However, the L1-regularization is easier to implement as feature selection does not need to be manual.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Multi-class Classification (The Realistic Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### i. Time Series Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making use of the varargs concept\n",
    "cv_factor, max_train_accuracy, best_l, total_l, result = 5, 0, 1, 20, []\n",
    "\n",
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']\n",
    "\n",
    "global max_train_accuracy, best_l\n",
    "max_train_accuracy, best_l = 0, 1\n",
    "# performing regression for l = 1 to 20\n",
    "for l in range(1, total_l + 1):\n",
    "    \n",
    "    print(\"Running multiclass classification with l = {}\".format(l), end = '\\r')\n",
    "    result.append(perform_classification(train_data, test_data, l, columns, selected_feature_names,\n",
    "                                       selected_features, cv_factor = cv_factor))\n",
    "    if result[-1][1] > max_train_accuracy:\n",
    "        max_train_accuracy = result[-1][1]\n",
    "        best_l = l\n",
    "\n",
    "result = pd.DataFrame(result, columns = ['L', 'Train Score', 'Train Error'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best model with l = {}, max_train_accuracy = {}\\n\".format(best_l, max_train_accuracy))\n",
    "print(\"Performing the analysis on test data with best l = {}\\n\".format(best_l))\n",
    "perform_classification(train_data, test_data, best_l, columns, selected_feature_names, \n",
    "                     selected_features, is_plot_roc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ii. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# making use of the varargs concept\n",
    "total_l, result = 20, []\n",
    "\n",
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']\n",
    "\n",
    "global max_train_accuracy, best_l\n",
    "max_train_accuracy, best_l = 0, 1\n",
    "\n",
    "# performing regression for l = 1 to 20\n",
    "for l in range(1, total_l + 1):\n",
    "    \n",
    "    print(\"Running Gaussian - naive bayes' classification with l = {}\".format(l), end = '\\r')\n",
    "    result.append(perform_classification(train_data, test_data, l, columns, selected_feature_names,\n",
    "                                       selected_features, is_gaussian = True))\n",
    "    if result[-1][1] > max_train_accuracy:\n",
    "        max_train_accuracy = result[-1][1]\n",
    "        best_l = l\n",
    "\n",
    "result = pd.DataFrame(result, columns = ['L', 'Train Score', 'Train Error'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Best model with l = {}, max_train_accuracy = {}\\n\".format(best_l, max_train_accuracy))\n",
    "print(\"Performing the analysis on test data with best l = {}\\n\".format(best_l))\n",
    "perform_classification(train_data, test_data, best_l, columns, selected_feature_names, \n",
    "                     selected_features, is_gaussian = True, is_plot_roc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-nomial priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making use of the varargs concept\n",
    "cv_factor, max_train_accuracy, best_l, total_l, result = 5, 0, 1, 20, []\n",
    "\n",
    "selected_feature_names = ['mean', 'median', 'std']\n",
    "selected_features = ['mean', '50%', 'std']\n",
    "\n",
    "global max_train_accuracy, best_l\n",
    "max_train_accuracy, best_l = 0, 1\n",
    "\n",
    "# performing regression for l = 1 to 20\n",
    "for l in range(1, total_l + 1):\n",
    "    \n",
    "    print(\"Running Multinomial priors - Naive Bayes' regression with l = {}\".format(l), end = '\\r')\n",
    "    result.append(perform_classification(train_data, test_data, l, columns, selected_feature_names,\n",
    "                                       selected_features, is_multinomial_prior = True))\n",
    "    if result[-1][1] > max_train_accuracy:\n",
    "        max_train_accuracy = result[-1][1]\n",
    "        best_l = l\n",
    "\n",
    "result = pd.DataFrame(result, columns = ['L', 'Train Score', 'Train Error'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best model with l = {}, max_train_accuracy = {}\\n\".format(best_l, max_train_accuracy))\n",
    "print(\"Performing the analysis on test data with best l = {}\\n\".format(best_l))\n",
    "perform_classification(train_data, test_data, best_l, columns, selected_feature_names, \n",
    "                   selected_features, is_multinomial_prior = True, is_plot_roc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the values the accuracy for both the methods, we can say that the Gaussian method is working better than Multinomial.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The area under the ROC curve (AUC) is relatively better for the Logistic Regression curve than any other classifer prior (Gaussian or Multinomial) and hence, I would say that Logistic Regression has better classification for the given dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 4.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem relates to the QDA model, in which the observations\n",
    "within each class are drawn from a normal distribution with a classspecific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature.\n",
    "Suppose that we have K classes, and that if an observation belongs\n",
    "to the kth class then X comes from a one-dimensional normal distribution, X ∼ N(µk, σ2\n",
    "k). Recall that the density function for the\n",
    "one-dimensional normal distribution is given in (4.16). Prove that in\n",
    "this case, the Bayes classifier is not linear. Argue that it is in fact\n",
    "quadratic.\n",
    "Hint: For this problem, you should follow the arguments laid out in\n",
    "Section 4.4.1, but without making the assumption that σ2\n",
    "1 = ... = σ2K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot%202023-10-16%20at%207.24.25%20PM.png](attachment:Screenshot%202023-10-16%20at%207.24.25%20PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Bayes classifier involves assigning an observation\n",
    "X = x to the class for which (4.17) is largest. Taking the log of (4.17) and\n",
    "rearranging the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![IMG_1237.jpg](attachment:IMG_1237.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![Screenshot%202023-10-16%20at%208.11.47%20PM.png](attachment:Screenshot%202023-10-16%20at%208.11.47%20PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since quadratic can not be ignored in this case, the Bayes classifier is not linear, it is in fact quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ISLR 4.8.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we wish to predict whether a given stock will issue a\n",
    "dividend this year (“Yes” or “No”) based on X, last year’s percent\n",
    "profit. We examine a large number of companies and discover that the\n",
    "mean value of X for companies that issued a dividend was X¯ = 10,\n",
    "while the mean for those that didn’t was X¯ = 0. In addition, the\n",
    "variance of X for these two sets of companies was ˆσ2 = 36. Finally,\n",
    "80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue\n",
    "a dividend this year given that its percentage profit was X = 4 last\n",
    "year.\n",
    "Hint: Recall that the density function for a normal random variable\n",
    "is f(x) = √\n",
    "1\n",
    "2πσ2 e−(x−µ)2/2σ2\n",
    ". You will need to use Bayes’ theorem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
